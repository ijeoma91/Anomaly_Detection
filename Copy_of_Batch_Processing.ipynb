{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWz+0MCCh5MZscP1ajRfKL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ijeoma91/Anomaly_Detection/blob/main/Copy_of_Batch_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zALL9iylX1aK",
        "outputId": "f1c8ccc0-5bba-41be-aa9b-48c195bb1e1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzXfFLJ6ZoDG",
        "outputId": "e30a28fa-61ca-4272-caf3-d13e1ab4117b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spark in /usr/local/lib/python3.10/dist-packages (0.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install cassandra-driver"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An3bbhCEbHHy",
        "outputId": "77e08894-71b3-4867-f6a1-263aa1a85288"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cassandra-driver in /usr/local/lib/python3.10/dist-packages (3.29.1)\n",
            "Requirement already satisfied: geomet<0.3,>=0.1 in /usr/local/lib/python3.10/dist-packages (from cassandra-driver) (0.2.1.post1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from geomet<0.3,>=0.1->cassandra-driver) (8.1.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from geomet<0.3,>=0.1->cassandra-driver) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datetime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2edECBicSjf",
        "outputId": "d1d459bb-3641-4480-a66e-be144caf29d4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datetime in /usr/local/lib/python3.10/dist-packages (5.5)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.10/dist-packages (from datetime) (6.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from datetime) (2023.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface->datetime) (67.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install timedelta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmpWze9Qhp27",
        "outputId": "61a361bb-829d-4138-a035-c41fb4d0f257"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timedelta in /usr/local/lib/python3.10/dist-packages (2020.12.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install jupyter-dash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dhyx2GBDKig9",
        "outputId": "984a3ee0-88bc-4ff0-89ea-180730c7949f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jupyter-dash\n",
            "  Downloading jupyter_dash-0.4.2-py3-none-any.whl (23 kB)\n",
            "Collecting dash (from jupyter-dash)\n",
            "  Downloading dash-2.16.1-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from jupyter-dash) (2.31.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from jupyter-dash) (2.2.5)\n",
            "Collecting retrying (from jupyter-dash)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from jupyter-dash) (7.34.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter-dash) (5.5.6)\n",
            "Collecting ansi2html (from jupyter-dash)\n",
            "  Downloading ansi2html-1.9.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from jupyter-dash) (1.6.0)\n",
            "Requirement already satisfied: Werkzeug<3.1 in /usr/local/lib/python3.10/dist-packages (from dash->jupyter-dash) (3.0.2)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash->jupyter-dash) (5.15.0)\n",
            "Collecting dash-html-components==2.0.0 (from dash->jupyter-dash)\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Collecting dash-core-components==2.0.0 (from dash->jupyter-dash)\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting dash-table==5.0.0 (from dash->jupyter-dash)\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from dash->jupyter-dash) (7.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from dash->jupyter-dash) (4.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from dash->jupyter-dash) (67.7.2)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask->jupyter-dash) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->jupyter-dash) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->jupyter-dash) (8.1.7)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter-dash) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter-dash) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter-dash) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter-dash) (6.3.3)\n",
            "Collecting jedi>=0.16 (from ipython->jupyter-dash)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->jupyter-dash) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->jupyter-dash) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->jupyter-dash) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->jupyter-dash) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->jupyter-dash) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->jupyter-dash) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->jupyter-dash) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->jupyter-dash) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->jupyter-dash) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->jupyter-dash) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->jupyter-dash) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from retrying->jupyter-dash) (1.16.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->jupyter-dash) (0.8.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask->jupyter-dash) (2.1.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->jupyter-dash) (0.7.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash->jupyter-dash) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash->jupyter-dash) (24.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->jupyter-dash) (0.2.13)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->dash->jupyter-dash) (3.18.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel->jupyter-dash) (5.7.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel->jupyter-dash) (23.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel->jupyter-dash) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.0->jupyter-client->ipykernel->jupyter-dash) (4.2.0)\n",
            "Installing collected packages: dash-table, dash-html-components, dash-core-components, retrying, jedi, ansi2html, dash, jupyter-dash\n",
            "Successfully installed ansi2html-1.9.1 dash-2.16.1 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 jedi-0.19.1 jupyter-dash-0.4.2 retrying-1.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBCSdecNRxB5",
        "outputId": "52172854-71ea-47c2-c578-332e5eb1e902"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.1.6)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ngrok authtoken 2eWmBka8I8O6dJGkz6BO9XhP2hN_c7jsE4CwP8fjbqwidHdE\n",
        "#from pyngrok import ngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHTBYGKrR5U-",
        "outputId": "cae574f3-4d12-4e29-866b-7e095dd3a072"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "from jupyter_dash import JupyterDash\n",
        "#import dash_core_components as dcc\n",
        "from dash import dcc\n",
        "from dash import html\n",
        "import dash_html_components as html"
      ],
      "metadata": {
        "id": "y9Ik1mq1Nr01"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5IPXP2veOetL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "packages = [\"pandas\", \"schedule\", \"cassandra-driver\"]\n",
        "for packages in packages:\n",
        "  os.system(f'pip install {packages}')"
      ],
      "metadata": {
        "id": "fQ4mrEAsZ794"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "#from datetime import datatime, timedelta\n",
        "import time\n",
        "import timedelta"
      ],
      "metadata": {
        "id": "HOjCUBl1bz9F"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_timestamp\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import *"
      ],
      "metadata": {
        "id": "4lA0h5ZujCfF"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"My Data Batch Processing\").getOrCreate()\n",
        "\n",
        "# Replace 'dataset path' with the actual path to your CSV file\n",
        "Batch_df = spark.read.csv(\"/content/iot_telemetry_data.csv\", header=True, sep=',')\n",
        "\n",
        "# Show the original DataFrame\n",
        "print(\"Original DataFrame:\")\n",
        "Batch_df.show()\n",
        "\n",
        "# Function to clean and uppercase column names\n",
        "def clean_and_uppercase(name):\n",
        "    # Remove unwanted characters (example: spaces and special characters)\n",
        "    cleaned_name = ''.join(e for e in name if e.isalnum())\n",
        "    # Make sure the first letter is uppercase\n",
        "    cleaned_name = cleaned_name[0].upper() + cleaned_name[1:]\n",
        "    return cleaned_name\n",
        "\n",
        "# Rename columns\n",
        "for col_name in Batch_df.columns:\n",
        "    new_col_name = clean_and_uppercase(col_name)\n",
        "    Batch_df = Batch_df.withColumnRenamed(col_name, new_col_name)\n",
        "\n",
        "# Show the modified DataFrame\n",
        "print(\"Modified DataFrame:\")\n",
        "Batch_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQeQAQpwBvBJ",
        "outputId": "828171c2-31fe-4fa1-82e3-2ead7851e041"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+--------------------+--------------------+--------------------+---------+--------------------+----------+--------------------+--------------------+\n",
            "|      \"ts,\"\"device\"\"|              \"\"co\"\"|        \"\"humidity\"\"|\"\"light\"\"|             \"\"lpg\"\"|\"\"motion\"\"|           \"\"smoke\"\"|           \"\"temp\"\"\"|\n",
            "+--------------------+--------------------+--------------------+---------+--------------------+----------+--------------------+--------------------+\n",
            "|\"1.59451209438597...|\"\"0.0049559386483...|            \"\"51.0\"\"|\"\"false\"\"|\"\"0.0076508222705...| \"\"false\"\"|\"\"0.0204112701224...|           \"\"22.7\"\"\"|\n",
            "|\"1.59451209473556...|\"\"0.0028400886071...|            \"\"76.0\"\"|\"\"false\"\"|\"\"0.0051143834009...| \"\"false\"\"|\"\"0.0132748367048...|\"\"19.700000762939...|\n",
            "|\"1.59451209807357...|\"\"0.0049760123404...|            \"\"50.9\"\"|\"\"false\"\"|\"\"0.0076732274063...| \"\"false\"\"|\"\"0.0204751255761...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451209958914...|\"\"0.0044030268296...|\"\"76.800003051757...| \"\"true\"\"|\"\"0.0070233371458...| \"\"false\"\"|\"\"0.0186282253770...|           \"\"27.0\"\"\"|\n",
            "|\"1.59451210176123...|\"\"0.0049673636419...|            \"\"50.9\"\"|\"\"false\"\"|\"\"0.0076635772823...| \"\"false\"\"|\"\"0.0204476208102...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451210446841...|\"\"0.0043910039545...|\"\"77.9000015258789\"\"| \"\"true\"\"|\"\"0.0070094585431...| \"\"false\"\"|\"\"0.0185889075400...|           \"\"27.0\"\"\"|\n",
            "|\"1.59451210544886...|\"\"0.0049760251182...|            \"\"50.9\"\"|\"\"false\"\"|\"\"0.0076732416602...| \"\"false\"\"|\"\"0.0204751662043...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451210686907...|\"\"0.0029381156266...|            \"\"76.0\"\"|\"\"false\"\"|\"\"0.0052414818417...| \"\"false\"\"|\"\"0.0136275211320...|\"\"19.700000762939...|\n",
            "|\"1.59451210827538...|\"\"0.0043454713595...|\"\"77.9000015258789\"\"| \"\"true\"\"|\"\"0.0069568023772...| \"\"false\"\"|\"\"0.0184397819021...|           \"\"27.0\"\"\"|\n",
            "|\"1.59451210913668...|\"\"0.0049702557644...|            \"\"50.9\"\"|\"\"false\"\"|\"\"0.0076668047981...| \"\"false\"\"|\"\"0.0204568196070...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451211279851...|\"\"0.0049602086559...|            \"\"50.9\"\"|\"\"false\"\"|\"\"0.0076555903135...| \"\"false\"\"|\"\"0.0204248581520...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451211528854...|\"\"0.0043830438373...|            \"\"78.0\"\"| \"\"true\"\"|\"\"0.0070002640007...| \"\"false\"\"|\"\"0.0185628624857...|           \"\"27.0\"\"\"|\n",
            "|\"1.59451211649826...|\"\"0.0049716449493...|            \"\"50.9\"\"|\"\"false\"\"|\"\"0.0076683548991...| \"\"false\"\"|\"\"0.0204612376699...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451211909801...|\"\"0.0044514976308...|            \"\"78.0\"\"| \"\"true\"\"|\"\"0.0070791835001...| \"\"false\"\"|\"\"0.0187864905644...|           \"\"27.0\"\"\"|\n",
            "|\"1.59451212018493...|\"\"0.0049645645184...|            \"\"50.9\"\"|\"\"false\"\"|\"\"0.0076604530556...| \"\"false\"\"|\"\"0.0204387166506...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451212278573...|\"\"0.0029050147565...|\"\"75.800003051757...|\"\"false\"\"|\"\"0.0051986974792...| \"\"false\"\"|\"\"0.0135087333295...|\"\"19.700000762939...|\n",
            "|\"1.59451212387261...|\"\"0.0049759834197...|            \"\"50.9\"\"|\"\"false\"\"|\"\"0.0076731951447...| \"\"false\"\"|\"\"0.0204750336202...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451212756018...|\"\"0.0049602086559...|            \"\"50.9\"\"|\"\"false\"\"|\"\"0.0076555903135...| \"\"false\"\"|\"\"0.0204248581520...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451212937415...|\"\"0.0044393227660...|\"\"77.9000015258789\"\"| \"\"true\"\"|\"\"0.0070651719347...| \"\"false\"\"|\"\"0.0187467746098...|           \"\"27.0\"\"\"|\n",
            "|\"1.59451213124780...|\"\"0.0049561192016...|            \"\"50.9\"\"|\"\"false\"\"|\"\"0.0076510239057...| \"\"false\"\"|\"\"0.0204118447333...|           \"\"22.6\"\"\"|\n",
            "+--------------------+--------------------+--------------------+---------+--------------------+----------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Modified DataFrame:\n",
            "+--------------------+--------------------+--------------------+---------+--------------------+---------+--------------------+--------------------+\n",
            "|            Tsdevice|                  Co|            Humidity|    Light|                 Lpg|   Motion|               Smoke|                Temp|\n",
            "+--------------------+--------------------+--------------------+---------+--------------------+---------+--------------------+--------------------+\n",
            "|\"1.59451209438597...|\"\"0.0049559386483...|            \"\"51.0\"\"|\"\"false\"\"|\"\"0.0076508222705...|\"\"false\"\"|\"\"0.0204112701224...|           \"\"22.7\"\"\"|\n",
            "|\"1.59451209473556...|\"\"0.0028400886071...|            \"\"76.0\"\"|\"\"false\"\"|\"\"0.0051143834009...|\"\"false\"\"|\"\"0.0132748367048...|\"\"19.700000762939...|\n",
            "|\"1.59451209807357...|\"\"0.0049760123404...|            \"\"50.9\"\"|\"\"false\"\"|\"\"0.0076732274063...|\"\"false\"\"|\"\"0.0204751255761...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451209958914...|\"\"0.0044030268296...|\"\"76.800003051757...| \"\"true\"\"|\"\"0.0070233371458...|\"\"false\"\"|\"\"0.0186282253770...|           \"\"27.0\"\"\"|\n",
            "|\"1.59451210176123...|\"\"0.0049673636419...|            \"\"50.9\"\"|\"\"false\"\"|\"\"0.0076635772823...|\"\"false\"\"|\"\"0.0204476208102...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451210446841...|\"\"0.0043910039545...|\"\"77.9000015258789\"\"| \"\"true\"\"|\"\"0.0070094585431...|\"\"false\"\"|\"\"0.0185889075400...|           \"\"27.0\"\"\"|\n",
            "|\"1.59451210544886...|\"\"0.0049760251182...|            \"\"50.9\"\"|\"\"false\"\"|\"\"0.0076732416602...|\"\"false\"\"|\"\"0.0204751662043...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451210686907...|\"\"0.0029381156266...|            \"\"76.0\"\"|\"\"false\"\"|\"\"0.0052414818417...|\"\"false\"\"|\"\"0.0136275211320...|\"\"19.700000762939...|\n",
            "|\"1.59451210827538...|\"\"0.0043454713595...|\"\"77.9000015258789\"\"| \"\"true\"\"|\"\"0.0069568023772...|\"\"false\"\"|\"\"0.0184397819021...|           \"\"27.0\"\"\"|\n",
            "|\"1.59451210913668...|\"\"0.0049702557644...|            \"\"50.9\"\"|\"\"false\"\"|\"\"0.0076668047981...|\"\"false\"\"|\"\"0.0204568196070...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451211279851...|\"\"0.0049602086559...|            \"\"50.9\"\"|\"\"false\"\"|\"\"0.0076555903135...|\"\"false\"\"|\"\"0.0204248581520...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451211528854...|\"\"0.0043830438373...|            \"\"78.0\"\"| \"\"true\"\"|\"\"0.0070002640007...|\"\"false\"\"|\"\"0.0185628624857...|           \"\"27.0\"\"\"|\n",
            "|\"1.59451211649826...|\"\"0.0049716449493...|            \"\"50.9\"\"|\"\"false\"\"|\"\"0.0076683548991...|\"\"false\"\"|\"\"0.0204612376699...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451211909801...|\"\"0.0044514976308...|            \"\"78.0\"\"| \"\"true\"\"|\"\"0.0070791835001...|\"\"false\"\"|\"\"0.0187864905644...|           \"\"27.0\"\"\"|\n",
            "|\"1.59451212018493...|\"\"0.0049645645184...|            \"\"50.9\"\"|\"\"false\"\"|\"\"0.0076604530556...|\"\"false\"\"|\"\"0.0204387166506...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451212278573...|\"\"0.0029050147565...|\"\"75.800003051757...|\"\"false\"\"|\"\"0.0051986974792...|\"\"false\"\"|\"\"0.0135087333295...|\"\"19.700000762939...|\n",
            "|\"1.59451212387261...|\"\"0.0049759834197...|            \"\"50.9\"\"|\"\"false\"\"|\"\"0.0076731951447...|\"\"false\"\"|\"\"0.0204750336202...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451212756018...|\"\"0.0049602086559...|            \"\"50.9\"\"|\"\"false\"\"|\"\"0.0076555903135...|\"\"false\"\"|\"\"0.0204248581520...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451212937415...|\"\"0.0044393227660...|\"\"77.9000015258789\"\"| \"\"true\"\"|\"\"0.0070651719347...|\"\"false\"\"|\"\"0.0187467746098...|           \"\"27.0\"\"\"|\n",
            "|\"1.59451213124780...|\"\"0.0049561192016...|            \"\"50.9\"\"|\"\"false\"\"|\"\"0.0076510239057...|\"\"false\"\"|\"\"0.0204118447333...|           \"\"22.6\"\"\"|\n",
            "+--------------------+--------------------+--------------------+---------+--------------------+---------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's find how many device that has the highest temperature thereby grouping\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "\n",
        "Batch_df.groupBy(\"Tsdevice\").agg(countDistinct(\"Temp\").alias(\"devices_count\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFj0h12TJj76",
        "outputId": "f77be400-3d35-4525-c684-37832f3899cc"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------+\n",
            "|            Tsdevice|devices_count|\n",
            "+--------------------+-------------+\n",
            "|\"1.59452496005098...|            1|\n",
            "|\"1.59455160255505...|            1|\n",
            "|\"1.59482533195824...|            1|\n",
            "|\"1.59483897429002...|            1|\n",
            "|\"1.59487127495136...|            1|\n",
            "|\"1.59487128679406...|            1|\n",
            "|\"1.59453095631002...|            1|\n",
            "|\"1.59457600258627...|            1|\n",
            "|\"1.59457845300001...|            1|\n",
            "|\"1.59459310240455...|            1|\n",
            "|\"1.59465516904282...|            1|\n",
            "|\"1.59466583917450...|            1|\n",
            "|\"1.59483756707918...|            1|\n",
            "|\"1.59459084007783...|            1|\n",
            "|\"1.59455132218290...|            1|\n",
            "|\"1.59465414743181...|            1|\n",
            "|\"1.59478061339207...|            1|\n",
            "|\"1.59481121126203...|            1|\n",
            "|\"1.59483560211041...|            1|\n",
            "|\"1.59460478521667...|            1|\n",
            "+--------------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Assuming batch_df is your DataFrame and \"Humidity\" is the column you want to cast to float\n",
        "Batch_df = Batch_df.withColumn(\"Humidity\", F.col(\"Humidity\").cast(\"float\"))\n",
        "\n",
        "# Show the result\n",
        "Batch_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWziH6siLZRV",
        "outputId": "7a5b6e81-0118-468f-b0a2-929987b70565"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------+---------+--------------------+---------+--------------------+--------------------+\n",
            "|            Tsdevice|                  Co|Humidity|    Light|                 Lpg|   Motion|               Smoke|                Temp|\n",
            "+--------------------+--------------------+--------+---------+--------------------+---------+--------------------+--------------------+\n",
            "|\"1.59451209438597...|\"\"0.0049559386483...|    NULL|\"\"false\"\"|\"\"0.0076508222705...|\"\"false\"\"|\"\"0.0204112701224...|           \"\"22.7\"\"\"|\n",
            "|\"1.59451209473556...|\"\"0.0028400886071...|    NULL|\"\"false\"\"|\"\"0.0051143834009...|\"\"false\"\"|\"\"0.0132748367048...|\"\"19.700000762939...|\n",
            "|\"1.59451209807357...|\"\"0.0049760123404...|    NULL|\"\"false\"\"|\"\"0.0076732274063...|\"\"false\"\"|\"\"0.0204751255761...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451209958914...|\"\"0.0044030268296...|    NULL| \"\"true\"\"|\"\"0.0070233371458...|\"\"false\"\"|\"\"0.0186282253770...|           \"\"27.0\"\"\"|\n",
            "|\"1.59451210176123...|\"\"0.0049673636419...|    NULL|\"\"false\"\"|\"\"0.0076635772823...|\"\"false\"\"|\"\"0.0204476208102...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451210446841...|\"\"0.0043910039545...|    NULL| \"\"true\"\"|\"\"0.0070094585431...|\"\"false\"\"|\"\"0.0185889075400...|           \"\"27.0\"\"\"|\n",
            "|\"1.59451210544886...|\"\"0.0049760251182...|    NULL|\"\"false\"\"|\"\"0.0076732416602...|\"\"false\"\"|\"\"0.0204751662043...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451210686907...|\"\"0.0029381156266...|    NULL|\"\"false\"\"|\"\"0.0052414818417...|\"\"false\"\"|\"\"0.0136275211320...|\"\"19.700000762939...|\n",
            "|\"1.59451210827538...|\"\"0.0043454713595...|    NULL| \"\"true\"\"|\"\"0.0069568023772...|\"\"false\"\"|\"\"0.0184397819021...|           \"\"27.0\"\"\"|\n",
            "|\"1.59451210913668...|\"\"0.0049702557644...|    NULL|\"\"false\"\"|\"\"0.0076668047981...|\"\"false\"\"|\"\"0.0204568196070...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451211279851...|\"\"0.0049602086559...|    NULL|\"\"false\"\"|\"\"0.0076555903135...|\"\"false\"\"|\"\"0.0204248581520...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451211528854...|\"\"0.0043830438373...|    NULL| \"\"true\"\"|\"\"0.0070002640007...|\"\"false\"\"|\"\"0.0185628624857...|           \"\"27.0\"\"\"|\n",
            "|\"1.59451211649826...|\"\"0.0049716449493...|    NULL|\"\"false\"\"|\"\"0.0076683548991...|\"\"false\"\"|\"\"0.0204612376699...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451211909801...|\"\"0.0044514976308...|    NULL| \"\"true\"\"|\"\"0.0070791835001...|\"\"false\"\"|\"\"0.0187864905644...|           \"\"27.0\"\"\"|\n",
            "|\"1.59451212018493...|\"\"0.0049645645184...|    NULL|\"\"false\"\"|\"\"0.0076604530556...|\"\"false\"\"|\"\"0.0204387166506...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451212278573...|\"\"0.0029050147565...|    NULL|\"\"false\"\"|\"\"0.0051986974792...|\"\"false\"\"|\"\"0.0135087333295...|\"\"19.700000762939...|\n",
            "|\"1.59451212387261...|\"\"0.0049759834197...|    NULL|\"\"false\"\"|\"\"0.0076731951447...|\"\"false\"\"|\"\"0.0204750336202...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451212756018...|\"\"0.0049602086559...|    NULL|\"\"false\"\"|\"\"0.0076555903135...|\"\"false\"\"|\"\"0.0204248581520...|           \"\"22.6\"\"\"|\n",
            "|\"1.59451212937415...|\"\"0.0044393227660...|    NULL| \"\"true\"\"|\"\"0.0070651719347...|\"\"false\"\"|\"\"0.0187467746098...|           \"\"27.0\"\"\"|\n",
            "|\"1.59451213124780...|\"\"0.0049561192016...|    NULL|\"\"false\"\"|\"\"0.0076510239057...|\"\"false\"\"|\"\"0.0204118447333...|           \"\"22.6\"\"\"|\n",
            "+--------------------+--------------------+--------+---------+--------------------+---------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#initial_data(batch_df)\n",
        "\n",
        "#spark.stop()"
      ],
      "metadata": {
        "id": "gNlxOklQNyL6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, functions as F\n",
        "from cassandra.cluster import Cluster\n",
        "from cassandra.auth import PlainTextAuthProvider\n",
        "\n",
        "def initialize_cassandra_session():\n",
        "    \"\"\"\n",
        "    Initializes and returns a Cassandra session.\n",
        "    Ensure the Cassandra cluster is accessible through the specified host and port.\n",
        "    \"\"\"\n",
        "    auth_provider = PlainTextAuthProvider(username='username', password='cassandra')\n",
        "    cluster = Cluster(['0.tcp.eu.ngrok.io'], port=18718, auth_provider=auth_provider)\n",
        "    session = cluster.connect()\n",
        "    return session\n",
        "\n",
        "def process_and_insert_data(dat, Batch_df):\n",
        "    \"\"\"\n",
        "    Processes the provided DataFrame and inserts the aggregated data into Cassandra.\n",
        "    \"\"\"\n",
        "    # Create aggregates\n",
        "    agg_df = Batch_df.groupBy(\"Tsdevice\").agg(\n",
        "        F.avg('Co').alias('avg_co'),\n",
        "        F.avg('Humidity').alias('avg_humidity'),\n",
        "        F.avg(' Lpg').alias('avg_lpg'),\n",
        "        F.avg('Smoke').alias('avg_smoke'),\n",
        "        F.sum(F.when(F.col('Light') == 'true', 1).otherwise(0)).alias('light_true'),\n",
        "        F.sum(F.when(F.col('Light') == 'false', 1).otherwise(0)).alias('light_false'),\n",
        "        F.sum(F.when(F.col('Motion') == 'true', 1).otherwise(0)).alias('motion_true'),\n",
        "        F.sum(F.when(F.col('Motion') == 'false', 1).otherwise(0)).alias('motion_false')\n",
        "    ).collect()\n",
        "\n",
        "    session = initialize_cassandra_session()\n",
        "    session.execute(\"CREATE KEYSPACE IF NOT EXISTS batch WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 3}\")\n",
        "    session.execute(\"\"\"CREATE TABLE IF NOT EXISTS batch.sensors (\n",
        "        device_id text PRIMARY KEY, date timestamp, Lpg float, Smoke float, Humidity float, Co float,\n",
        "        light_true int, light_false int, motion_true int, motion_false int)\"\"\")\n",
        "\n",
        "    # Insert data\n",
        "    for row in agg_df:\n",
        "        session.execute(\n",
        "            \"\"\"\n",
        "            INSERT INTO batch.sensors (device_id, date, lpg, smoke, humidity, co, light_true, light_false, motion_true, motion_false)\n",
        "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
        "            \"\"\",\n",
        "            (row['Tsdevice'], dat, row['avg_lpg'], row['avg_smoke'], row['avg_humidity'], row['avg_co'],\n",
        "             row['light_true'], row['light_false'], row['motion_true'], row['motion_false'])\n",
        "        )\n",
        "\n",
        "    print(\"Data inserted into Cassandra for date:\", dat)\n",
        "\n",
        "def main(spark, Batch_df):\n",
        "    # Example usage with a specific date\n",
        "    process_and_insert_data('2024-04-02', Batch_df)  # Replace '2023-01-01' with your actual date variable\n",
        "\n",
        "\n",
        "\n",
        "if Batch_df== \"_main_\":\n",
        "    spark = SparkSession.builder.appName(\"My Data Batch Processing\").getOrCreate()\n",
        "    # Assuming you've loaded your DataFrame 'df' here\n",
        "    # df = spark.read.csv(\"your_csv_file_path.csv\", header=True, inferSchema=True)\n",
        "\n",
        "\n",
        "    main(spark, Batch_df)  # Pass 'df' as an argument to 'main'"
      ],
      "metadata": {
        "id": "3z7bWqu5Ud8X"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 2 Feedback\n",
        "\n",
        "Adding Data Validation and Robust error handling into the PySpark\n",
        "\n",
        "Incorporting data validation and robust error handling into the PySpark code can significantly improve its reliability and maintainability, especially when dealing with large dataset and complex transformations.\n"
      ],
      "metadata": {
        "id": "Z3key5fgdoNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Validation\n",
        "\n",
        "Data validation involves checking that the data meets certain criteria before processing it. This is curcial to prevent errors during execution and esure the quality of the outputs.\n",
        "\n",
        "Firstly, Data Enhancement Validation\n",
        "\n",
        "a. Schema Validation: Checking that the DataFrame contains the expected columns with the correct data types\n"
      ],
      "metadata": {
        "id": "JwSDwHdMe0GD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.utils import AnalysisException\n",
        "\n",
        "# Correct the dictionary to match the expected types\n",
        "required_columns = {\n",
        "    'Tsdevice': 'float',  # Corrected to 'float'\n",
        "    'Co': 'float',\n",
        "    'Humidity': 'float',\n",
        "    'Lpg': 'float',\n",
        "    'Smoke': 'float',\n",
        "    'Light': 'string',\n",
        "    'Motion': 'string'\n",
        "}\n",
        "\n",
        "def validate_schema(Batch_df, required_columns):\n",
        "    try:\n",
        "        for column, dtype in required_columns.items():\n",
        "            # Validate if the DataFrame schema's data type matches the expected type\n",
        "            if Batch_df.schema[column].dataType.simpleString() != dtype:\n",
        "                # Raise an error if there's a mismatch\n",
        "                raise ValueError(f\"Column {column} must be of type {dtype}, found {Batch_df.schema[column].dataType.simpleString()}\")\n",
        "    except AnalysisException as e:\n",
        "        # Collect all column names to indicate which are expected in the DataFrame\n",
        "        missing_cols = ', '.join(required_columns.keys())\n",
        "        raise ValueError(f\"DataFrame is missing required columns: {missing_cols}\") from e\n",
        "\n",
        "# Example usage\n",
        "#validate_schema(Batch_df, required_columns)"
      ],
      "metadata": {
        "id": "OB4DDD_udkeg"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**### Data Completeness:**\n",
        "\n",
        "Ensuring there are no missing values in critical columns before processing the Dataset"
      ],
      "metadata": {
        "id": "Qz5siIrg1qgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_missing_values(Batch_df, columns):\n",
        "    for column in columns:\n",
        "        missing_count = Batch_df.filter(F.col(column).isNull()).count()\n",
        "        if missing_count > 0:\n",
        "            raise ValueError(f\"Column {column} contains {missing_count} missing values\")\n",
        "\n",
        "check_missing_values(Batch_df, ['Tsdevice', 'Co', 'Lpg', 'Smoke', 'Light', 'Motion'])"
      ],
      "metadata": {
        "id": "tCZA2XIC1pQ6"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Error Handling\n",
        "\n",
        "Robust error handling ensures the application behaves predictably under various failure scenarios. This includes handling expections gracefully and possibly retrying operation if appropriate.\n",
        "\n",
        "Example Enhancements for Error Handling:\n",
        "\n",
        "1. Handling Database Connection and Query Failure\n",
        "  a. We will implement retries for database operations.\n",
        "  b. Use try-except blocks to handle and log error during database interations\n",
        "  "
      ],
      "metadata": {
        "id": "yjiToj1P3OqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6PDa-Fn2UnuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def execute_with_retries(session, query, parameters, max_retries=3):\n",
        "    retry_count = 0\n",
        "    while retry_count < max_retries:\n",
        "        try:\n",
        "            session.execute(query, parameters)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            retry_count += 1\n",
        "            print(f\"Failed to execute query, attempt {retry_count}/{max_retries}. Error: {e}\")\n",
        "            time.sleep(2)  # wait 2 seconds before retrying\n",
        "            if retry_count == max_retries:\n",
        "                raise"
      ],
      "metadata": {
        "id": "Nem2OPLU4_nJ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Wraping Processing in a Try-Except Block\n",
        "\n",
        "This block can catch unexpected errors during data processing and log thm."
      ],
      "metadata": {
        "id": "ejZ32RRg5QQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_and_insert_data(dat, Batch_df):\n",
        "    try:\n",
        "        validate_schema(Batch_df, required_columns)\n",
        "        check_missing_values(Batch_df, ['Tsdevice', 'Co', 'Humidity', 'Lpg', 'Smoke', 'Light', 'Lotion'])\n",
        "        agg_df = Batch_df.groupBy(\"Tsdevice\").agg(...)\n",
        "        session = initialize_cassandra_session()\n",
        "        ...\n",
        "        for index, row in pandas_df.iterrows():\n",
        "            execute_with_retries(session, insert_query, (row['Tsdevice'], dat, ...))\n",
        "        print(\"Data inserted into Cassandra for date:\", dat)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing data: {e}\")\n",
        "        raise\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        spark = SparkSession.builder.appName(\"My Data Batch Processing\").getOrCreate()\n",
        "        ...\n",
        "        process_and_insert_data('your_date_here', Batch_df)\n",
        "    except Exception as general_error:\n",
        "        print(f\"An error occurred: {general_error}\")"
      ],
      "metadata": {
        "id": "QRN_t8dS5yNZ"
      },
      "execution_count": 51,
      "outputs": []
    }
  ]
}